#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Doclingâ€¯+â€¯EasyOCR extraction for large Koreanâ€¯/â€¯English medical PDFs on an NVIDIAâ€¯L4.
Optimised for speed (skip OCR on bornâ€‘digital pages) and memory (batched rendering).
"""

from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime

import torch  # sanityâ€‘check GPU visibility
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import (
    PdfPipelineOptions,
    EasyOcrOptions,
    AcceleratorOptions,
    AcceleratorDevice,
    TableFormerMode,
)
from docling.datamodel.base_models import InputFormat
from docling.datamodel.settings import settings

from docling_core.types.doc import ImageRefMode

# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PDF_SOURCE_DIR = Path("medical_pdfs/bookData")
OUTPUT_DIR = Path("extracted_data/finalExtracted")

EXPORT_JSON = True    # dump structured JSON alongside Markdown
RECURSIVE   = False   # True â†’ recurse into subâ€‘folders
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def log_result(result, pdf_name: str) -> None:
    """Print Docling pageâ€‘level warnings / errors (if any)."""
    if getattr(result, "warnings", None):
        print(f"âš ï¸  {pdf_name}: {len(result.warnings)} warnings")
        for w in result.warnings:
            print("    â€¢", w)
    if getattr(result, "errors", None):
        print(f"ðŸ›‘  {pdf_name}: {len(result.errors)} errors")
        for e in result.errors:
            print("    â€¢", e)


def extract_pdf_data(pdf_path: Path,
                     output_root: Path,
                     converter: DocumentConverter) -> None:
    """Extract text, tables, and images from one PDF."""
    print(f"ðŸ”„  Processing: {pdf_path.relative_to(PDF_SOURCE_DIR)}")

    pdf_output_dir = output_root / pdf_path.stem
    # images_dir     = pdf_output_dir / "images"
    # images_dir.mkdir(parents=True, exist_ok=True)
    pdf_output_dir.mkdir(parents=True, exist_ok=True)

    try:
        result = converter.convert(source=str(pdf_path))

        total_secs = (
            result.timings.get("pipeline_total").times[0]
            if "pipeline_total" in result.timings else None
        )

        # Markdown (humanâ€‘readable) export
        md_path      = pdf_output_dir / f"{pdf_path.stem}.md"
        # md_content = result.document.export_to_markdown()
        # md_path.write_text(md_content, encoding="utf-8")
        # result.document.save_as_markdown(md_path) # Using save_as_markdown for consistency
        result.document.save_as_markdown(
            md_path,
            artifacts_dir=pdf_output_dir / "images",   # write PNGs here
            image_mode=ImageRefMode.REFERENCED         # use normal ![](images/xxx.png) links
        )


        # JSON (machineâ€‘readable) export
        if EXPORT_JSON:
            json_path = pdf_output_dir / f"{pdf_path.stem}.json"
            # result.document.save_as_json(json_path, image_mode=ImageRefMode.EMBEDDED)
            result.document.save_as_json(
                json_path,
                image_mode=ImageRefMode.REFERENCED,
                artifacts_dir=pdf_output_dir / "images"   # Docling will create & populate
            )

        log_result(result, pdf_path.name)
        print(f"âœ…  Done ({total_secs:.2f}s)" if total_secs else "âœ…  Done")

    except Exception as exc:
        print(f"âŒ  Failed on {pdf_path.name}: {exc}")


def build_converter() -> DocumentConverter:
    """Configure a Docling converter for bilingual OCR on CUDA."""
    ocr_cfg = EasyOcrOptions(
        lang=["ko", "en"],
        force_full_page_ocr=False,   # let Docling reuse embedded text when present
    )

    pipe_opts = PdfPipelineOptions(
        do_ocr=True,
        do_table_structure=True,
        ocr_options=ocr_cfg,
        accelerator_options=AcceleratorOptions(device=AcceleratorDevice.CUDA),
        pages_per_batch=30,          # prevents GPU OOM on 3000â€‘page tomes
    )
    # Highâ€‘accuracy table mode (toggle to FAST if speed > fidelity)
    pipe_opts.table_structure_options.mode = TableFormerMode.ACCURATE

    # pipe_opts.num_cpu_workers = 4

    fmt_opts = {InputFormat.PDF: PdfFormatOption(pipeline_options=pipe_opts)}
    return DocumentConverter(format_options=fmt_opts)


def main() -> None:
    """Entry point: set up converter, iterate over PDFs, extract."""
    # Hard failure if CUDA isnâ€™t visible (avoids silent CPU fallback)
    assert torch.cuda.is_available(), "CUDA not detected â€” check PyTorch/CUDA install"

    if not PDF_SOURCE_DIR.exists():
        raise FileNotFoundError(f"Source directory not found: {PDF_SOURCE_DIR}")

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    settings.debug.profile_pipeline_timings = True

    converter = build_converter()
    print("ðŸš€  Converter ready â€“ starting extractionâ€¦\n")

    pdf_iter = (
        PDF_SOURCE_DIR.rglob("*.pdf") if RECURSIVE
        else PDF_SOURCE_DIR.glob("*.pdf")
    )
    for pdf_file in pdf_iter:
        extract_pdf_data(pdf_file, OUTPUT_DIR, converter)

    print("\nâœ…  Extraction completed at",
          datetime.now().strftime("%Yâ€‘%mâ€‘%d %H:%M:%S"))


if __name__ == "__main__":
    # Uncomment if multiple GPUs and you want to pin to a specific one
    # os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    main()
